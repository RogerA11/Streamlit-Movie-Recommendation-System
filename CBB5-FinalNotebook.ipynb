{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPSxSkLkTTG7"
   },
   "source": [
    "\n",
    "<font size=\"8\"> **Movie Recommendation 2022**</font>\n",
    "\n",
    "© Explore Data Science Academy\n",
    "** **\n",
    "<br />\n",
    "\n",
    "<img src=\"https://explore-datascience.net/images/images_admissions2/main-logo.jpg\" width=600 height=100 />\n",
    "\n",
    "<br />\n",
    "\n",
    "** **\n",
    "\n",
    "### Team Members\n",
    "\n",
    "\n",
    "\n",
    "*   Joshan Dooki\n",
    "*   Wade Grant Jacobs\n",
    "*   Gabrielle Peria\n",
    "*   Njabulo Mkhwanazi\n",
    "*   William Mxh\n",
    "*   Roger Arendse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e5580d4"
   },
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Understand the Data</a>\n",
    "\n",
    "<a href=#four>4. Clean Data (EDA)</a>\n",
    "\n",
    "<a href=#five>5. Data Preprocessing (EDA)</a> \n",
    "\n",
    "<a href=#six>6. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#seven>7. Data Engineering</a>\n",
    "\n",
    "<a href=#eight>8. Model creation </a>\n",
    "\n",
    "<a href=#nine>9. Model Improvements</a>\n",
    "\n",
    "<a href=#ten>10. Model Evaluation</a>\n",
    "\n",
    "<a href=#eleven>11. Discuss chosen methods logic</a>\n",
    "\n",
    "<a href=#twelve>12. Conclusions</a>\n",
    "\n",
    "<a href=#thirteen>13. Recommendations</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0c22280"
   },
   "source": [
    "<br />\n",
    "\n",
    "<center> <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRQtJKAki_4xOa-PVen8OMtvoKSGHslY3Ql4uyCPhtxbs2DtQjobgoxntaA4cI49OZkfb4&usqp=CAU\" width=400 height=300 />\n",
    "\n",
    "<br />\n",
    "    \n",
    "Comet is a great tool for model versioning and experimentation as it records the parameters and conditions from each of your experiements- allowing you to reproduce your results, or go back to a previous version of your experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04e21c4d"
   },
   "source": [
    "To begin with, let us install as illustrated below since it we don't have it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T13:00:48.830962Z",
     "iopub.status.busy": "2022-07-28T13:00:48.830220Z"
    },
    "id": "7badb5d8",
    "outputId": "d8f8f6ad-1161-45b5-f574-ece6b3a68b1d"
   },
   "outputs": [],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "781e2ebc"
   },
   "source": [
    "Firstly, Let us import Experiment at the top of your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50055f06"
   },
   "outputs": [],
   "source": [
    "# import comet_ml at the top of the file\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10ed234e"
   },
   "source": [
    "Now we can link workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "832c549d",
    "outputId": "c81ba3b5-0683-4c3a-b87e-f580607e8f5b"
   },
   "outputs": [],
   "source": [
    "# import comet_ml at the top of your file\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(api_key=\"zlgBAmM77HkBRmWSBEiUL3HIx\",project_name=\"black-box\",workspace=\"njabulomkhwanazi52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "322ae825"
   },
   "source": [
    "\n",
    "### Honour Code\n",
    "\n",
    "We Team Cbb5, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "\n",
    "### Predict Overview:\n",
    "In today’s technology driven world, recommender systems are socially and economically critical to ensure that individuals can make optimised choices surrounding the content they engage with on a daily basis. One application where this is especially true is movie recommendations; where intelligent algorithms can help viewers find great titles from tens of thousands of options.\n",
    "\n",
    "With this context, EDSA is challenging you to construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed, based on their historical preferences.\n",
    "\n",
    "Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being personalised recommendations - generating platform affinity for the streaming services which best facilitates their audience's viewing. See figure below adapted from [Kaggle](https://www.kaggle.com/competitions/edsa-movie-recommendation-2022/overview).\n",
    "\n",
    "<br />\n",
    "\n",
    "<center> <img src=\"https://research.aimultiple.com/wp-content/uploads/2017/08/recommendation-system.png\" width=600 height=500 />\n",
    "\n",
    "<br />\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db4eb171"
   },
   "source": [
    "### Introduction\n",
    "The movie industry has significantly evolved over the past decade, mainly driven by rapid technological advancements. In 1990's ad early 2000's a person would walk into a movie store (such as Mr. Video) and pick out a movie based  on his/her preferences. These preferences ranged from popularity and genre to lead actors/actresses and film directors. With the help of technology, companies participating in this industry are able to capitalize on this $91.83 Billion industry by creating reccomendation systems to ensure their customers recieve the most relevant content for their entertainment.\n",
    "\n",
    "Considering a company like Netflix which has 222 million active subscribers (as of 2022), creating personalized recommendations for each individual is no simple task. This is where data scientists come into play. Data scientists use a series of computer science tools and algorthims to extract all the relevant information required to provide each user with content that best suits their preferences.\n",
    "\n",
    "| ⚡ This notebook provides a detailed stepwise breakdown on the creation of a movie recommendation system ⚡ |\n",
    "| :--------------------------- |\n",
    "\n",
    "\n",
    "* What is a Movie Recommendation system?\n",
    "\n",
    "A recommender system is an intelligent system that predicts the rating and preferences of users on products. The primary application of recommender systems is finding a relationship between user and products in order to maximise the user-product engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc4b4089"
   },
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DjZUBkNTfVDr",
    "outputId": "3d2931dd-1486-4c97-f8d5-c060d7dd599b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\joshd\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.2 in c:\\users\\joshd\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\joshd\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.7.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\joshd\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.16.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "    Running setup.py install for scikit-surprise: started\n",
      "    Running setup.py install for scikit-surprise: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\joshd\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\joshd\\\\AppData\\\\Local\\\\Temp\\\\pip-install-88n5a7ng\\\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\joshd\\\\AppData\\\\Local\\\\Temp\\\\pip-install-88n5a7ng\\\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\joshd\\AppData\\Local\\Temp\\pip-wheel-8um_k_4v'\n",
      "       cwd: C:\\Users\\joshd\\AppData\\Local\\Temp\\pip-install-88n5a7ng\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\n",
      "  Complete output (52 lines):\n",
      "  C:\\Users\\joshd\\anaconda3\\lib\\site-packages\\setuptools\\dist.py:757: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "  creating build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-3.9\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\joshd\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\joshd\\\\AppData\\\\Local\\\\Temp\\\\pip-install-88n5a7ng\\\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\joshd\\\\AppData\\\\Local\\\\Temp\\\\pip-install-88n5a7ng\\\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\joshd\\AppData\\Local\\Temp\\pip-record-phfwscyu\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\joshd\\anaconda3\\Include\\scikit-surprise'\n",
      "         cwd: C:\\Users\\joshd\\AppData\\Local\\Temp\\pip-install-88n5a7ng\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\n",
      "    Complete output (54 lines):\n",
      "    C:\\Users\\joshd\\anaconda3\\lib\\site-packages\\setuptools\\dist.py:757: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "      warnings.warn(\n",
      "    running install\n",
      "    C:\\Users\\joshd\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      warnings.warn(\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\accuracy.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\dataset.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\dump.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\reader.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\trainset.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\utils.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\__init__.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\__main__.py -> build\\lib.win-amd64-3.9\\surprise\n",
      "    creating build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "    copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "    copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "    copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "    copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-3.9\\surprise\\model_selection\n",
      "    creating build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    running egg_info\n",
      "    writing scikit_surprise.egg-info\\PKG-INFO\n",
      "    writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "    writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "    writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "    writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "    reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    adding license file 'LICENSE.md'\n",
      "    writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "    copying surprise\\similarities.c -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\similarities.pyx -> build\\lib.win-amd64-3.9\\surprise\n",
      "    copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-3.9\\surprise\\prediction_algorithms\n",
      "    running build_ext\n",
      "    building 'surprise.similarities' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\joshd\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\joshd\\\\AppData\\\\Local\\\\Temp\\\\pip-install-88n5a7ng\\\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\joshd\\\\AppData\\\\Local\\\\Temp\\\\pip-install-88n5a7ng\\\\scikit-surprise_bebd7e00d0414883adae0604a2a14f45\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\joshd\\AppData\\Local\\Temp\\pip-record-phfwscyu\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\joshd\\anaconda3\\Include\\scikit-surprise' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install scikit-surprise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# !conda install -y -c conda-forge scikit-surprise # If you use conda on a non-Colab environment\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#from surprise.model_selection import cross_validate\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader, Dataset, SVD\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# This Python 3 environment comes with many helpful analytics libraries installed\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# For example, here's several helpful packages to load\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import timeit\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#modelling \n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "!pip install scikit-surprise\n",
    "# !conda install -y -c conda-forge scikit-surprise # If you use conda on a non-Colab environment\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Reader, Dataset, SVD\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ba2b44c"
   },
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-_0dwDbMMMs"
   },
   "source": [
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*5qQEAEAZsCoOpptAeN4RBA.png\" width=500 height=200 />\n",
    "\n",
    "<br />\n",
    "\n",
    "convenient way to import data from Kaggle directly to Google Colab notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxLRb-KhN5Zb"
   },
   "source": [
    "let’s install the Kaggle package that will be used for importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5JYtGhlN9oA"
   },
   "outputs": [],
   "source": [
    " !pip install -q kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQFEZOdlRPeb"
   },
   "source": [
    "Then, save the json file with your credentials on your computer and upload this file to Colab using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AR84gU0dRSk8",
    "outputId": "a5248a52-2152-4959-edd7-21a9eed54d05"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_DcX0Z6Sicv",
    "outputId": "55571282-3451-4b5f-94c9-930b3cc1cacc"
   },
   "outputs": [],
   "source": [
    "#Create a Kaggle directory\n",
    "#! mkdir ~/.kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJSNucxQyWno"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvTs7CZwSkTN"
   },
   "outputs": [],
   "source": [
    "# copy the jason file to kaggle folder\n",
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x_hEVfYSxM7"
   },
   "outputs": [],
   "source": [
    "# Permissions for the jason file to act\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gF8nzNAS3B0"
   },
   "outputs": [],
   "source": [
    "#  # To list all the dataset in kaggle\n",
    "#  ! kaggle datasets list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiVIJFhLTHX4"
   },
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vYooYIlS8hH",
    "outputId": "efa9c388-faca-401a-fe5a-b1d648d7fb1d"
   },
   "outputs": [],
   "source": [
    "#!kaggle competitions download -c edsa-movie-recommendation-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdauV5V0Txis",
    "outputId": "cc0f8e1a-a30d-4ebd-dc85-fb458812ce4f"
   },
   "outputs": [],
   "source": [
    "# Now we have to unzip the file \n",
    "#!unzip edsa-movie-recommendation-2022.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0AkHhKegd_x"
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "genome_score = pd.read_csv('../input/edsa-movie-recommendation-2022/genome_scores.csv')\n",
    "genome_tags = pd.read_csv('../input/edsa-movie-recommendation-2022/genome_tags.csv')\n",
    "imdb_data = pd.read_csv('../input/edsa-movie-recommendation-2022/imdb_data.csv')\n",
    "links = pd.read_csv('../input/edsa-movie-recommendation-2022/links.csv')\n",
    "movies = pd.read_csv('../input/edsa-movie-recommendation-2022/movies.csv')\n",
    "tags = pd.read_csv('../input/edsa-movie-recommendation-2022/tags.csv')\n",
    "test = pd.read_csv('../input/edsa-movie-recommendation-2022/test.csv')\n",
    "train = pd.read_csv('../input/edsa-movie-recommendation-2022/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f32e4e5"
   },
   "source": [
    "<a id=\"three\"></a>\n",
    "# 3. Understand the Data\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97cbc037"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/Relationships.png?token=GHSAT0AAAAAABXBQYEH7KKKIJESRUUAUUGOYXCVURA\" width=1000 height=1000 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "216e92c0",
    "outputId": "777673f3-42ef-43eb-ce3d-587ec57cae68"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0c86490",
    "outputId": "cdf14a91-e96c-4504-b0bf-8a9e4a9857a6"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cc9ea8",
    "outputId": "c962596f-ad9d-470f-a7a6-f8c07a6a9fb4"
   },
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6f24c66",
    "outputId": "272b764e-d35f-4974-ace3-9fdcadcf3faf"
   },
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5335dc06",
    "outputId": "6463bdf6-ae5f-42e3-f0a5-99453c0adbd2"
   },
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58c5bf1b",
    "outputId": "49618967-9870-428c-8516-1391009542eb"
   },
   "outputs": [],
   "source": [
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6870ed08",
    "outputId": "0ab45f2c-7b0f-41d7-cb1f-0248da66a7bd"
   },
   "outputs": [],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88d7b758",
    "outputId": "7b182a0e-faf6-434e-c189-1dc2eb08e84e"
   },
   "outputs": [],
   "source": [
    "genome_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "146e1d72",
    "outputId": "ba5549fb-9a37-4c25-c5ee-78b7646c1269"
   },
   "outputs": [],
   "source": [
    "genome_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45065532",
    "outputId": "1b66cb61-c980-4d12-9ba4-c10f336e3304"
   },
   "outputs": [],
   "source": [
    "genome_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf20f7eb",
    "outputId": "61a2d99c-aab9-4f9f-c7ba-8dad93131785"
   },
   "outputs": [],
   "source": [
    "genome_score.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c859bed9"
   },
   "source": [
    "<a id=\"four\"></a>\n",
    "# 4. Cleaning Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sPKdSYXVvHl"
   },
   "source": [
    "Dataframe obviously has a relation between all the explored datasets as it have 'movieId' column in all the datasets. It was important to have such relation so that we would be able to concatenate the datasets. We can do so for train, movies and imdb_data dataframes before we clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoagpAf4V7xi"
   },
   "outputs": [],
   "source": [
    "dfs = [train, movies, imdb_data]\n",
    "import functools as ft\n",
    "data = ft.reduce(lambda left, right: pd.merge(left, right, on='movieId'), dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOdp6ysVW8S4"
   },
   "source": [
    "Now let us view our new data and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf1qXh4eW5s0",
    "outputId": "2e8fa157-5012-49f3-ef91-abfa65728d2b"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmRAXh3Hctnh"
   },
   "source": [
    "The New \"data\" will give us more insights on Exploratory data Analysis (EDA) in section still to be covered in this notebook. So, let us clean this data if any missing data or any incorrect format is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g14Zz7vdi0n",
    "outputId": "b33acaad-1a00-41ae-be0f-c66eea7ef074"
   },
   "outputs": [],
   "source": [
    "# Firtly, let look at info of our dataset.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp8quN91edKj"
   },
   "source": [
    "Notice, that we have varying # of non-null counts for some features. Hence this can show us that there are missing values in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfJ-FEOxhHSO",
    "outputId": "2ba08712-87d1-4052-8f10-3195547d1e89"
   },
   "outputs": [],
   "source": [
    "# The sum to count the NaN values For dataset:\n",
    "((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-IS-T-iiQ0_"
   },
   "source": [
    "In the info displayed above we see the presentage of null values mostly below 75 percent, hence we fill null values with mean and mode for numerical and categorical values respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr2Tgpa8jA_I"
   },
   "outputs": [],
   "source": [
    "# for tltle_cast as it categorical feature we employ mode\n",
    "data['title_cast'] = data['title_cast'].fillna(data['title_cast'].mode()[0])\n",
    "# for director as it categorical feature we employ mode\n",
    "data['director'] = data['director'].fillna(data['director'].mode()[0])\n",
    "# for runtime as it numerical feature we employ mean\n",
    "data['runtime'] = data['runtime'].fillna(data['runtime'].mean())\n",
    "# for budget as it categorical feature we employ mode\n",
    "data['budget'] = data['budget'].fillna(data['budget'].mode()[0])\n",
    "# for plot_keywords as it categorical feature we employ mode\n",
    "data['plot_keywords'] = data['plot_keywords'].fillna(data['plot_keywords'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TfI0piAmRSl"
   },
   "source": [
    "Now let us veiw our dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2liras8AmYx-",
    "outputId": "fe1fedf0-1936-40dc-a550-d5a2acbcf535"
   },
   "outputs": [],
   "source": [
    "# The sum to count the NaN values For dataset:\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c00f6a68"
   },
   "source": [
    "<a id=\"five\"></a>\n",
    "# 5.  Data Preprocessing\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKgcqRGHvta0"
   },
   "source": [
    "In our data, we can extract the year a specific movie was released. Thus, this is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6_OB5a7nGCa"
   },
   "outputs": [],
   "source": [
    "data['release_year']=data['title'].str[-5:-1] #extracting released year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hsilXDd9Umi"
   },
   "source": [
    "Let us explore the new features in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el3uVkrA9clr",
    "outputId": "76107ef8-7a2b-47b0-c811-022998005500"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22bc1efa"
   },
   "source": [
    "<a id=\"six\"></a>\n",
    "# 6. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaqKAfLCNYEI"
   },
   "source": [
    "Each machine learning algorithm requires different way to explore the dataset to get valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ-ksdhpOUEP"
   },
   "source": [
    "**Unique Counts and Data Shape**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJML7rSyOi4W"
   },
   "source": [
    "Firstly, an overview of how many distinct users and movies are included in the dataset. This can be easily achieved using df.nunique(axis = 0)and then plot it in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTeWUaQvOiME",
    "outputId": "c0d1d050-08cf-41c4-c325-7dedd53a44a1"
   },
   "outputs": [],
   "source": [
    "# obtain unique counts for each column in the dataset\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "print(data.nunique(axis=0))\n",
    "ax = sns.barplot(x=data.columns, y=data.nunique(axis=0), palette='Blues_d')\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45)\n",
    "plt.title('unique counts for each column in the dataset', size=20)\n",
    "plt.xlabel('Features', size=15)\n",
    "plt.ylabel('Unique counts', size=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ty_ajJhSequ"
   },
   "source": [
    "**Univariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSARk-_vSi38"
   },
   "source": [
    "Univariate analysis — the analysis of one feature at a time — helps us to better understand three questions:\n",
    "\n",
    "* what are the movies with most reviews?\n",
    "* who are the users that provide most reviews?\n",
    "* how does the distribution looks like for ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpVMXi_wSyW3",
    "outputId": "dc81042f-5266-45fc-c34f-fe55a4c21a8c"
   },
   "outputs": [],
   "source": [
    "# univariate analysis\n",
    "plt.figure(1, figsize = (16,4))\n",
    "data['movieId'].value_counts()[:50].plot(kind = 'bar', color=\"indigo\") #take top 50 movies\n",
    "plt.title('Movies with most reviews', size=20)\n",
    "plt.xlabel('MovieId', size=15)\n",
    "plt.ylabel('# of reviews', size=15)\n",
    "plt.figure(2, figsize = (16,4))\n",
    "data['userId'].value_counts()[:50].plot(kind = 'bar', color=\"red\") #take top 50 users\n",
    "plt.title('Users that provide most reviews', size=20)\n",
    "plt.xlabel('UserId', size=15)\n",
    "plt.ylabel('# of reviews', size=15)\n",
    "plt.figure(3, figsize = (15,10))\n",
    "data['rating'].plot(kind = 'hist', color=\"violet\")\n",
    "plt.title('The distribution of ratings', size=20)\n",
    "plt.xlabel('rating', size=15)\n",
    "plt.ylabel('Frequency', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a0045ed"
   },
   "source": [
    "Some of the insights we can draw from these distribution charts are: \n",
    "\n",
    "1. ratings are not evenly distributed among movies and the most rated movies is \"318\" which has no more than 35000 ratings; \n",
    "\n",
    "2. ratings are not evenly distributed across users and users at most provided around 7000 ratings; \n",
    "\n",
    "3. most people are likely to give a rating around 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swQrHWQpGSl8"
   },
   "source": [
    "Let’s create a ratings dataframe with average rating and number of ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3PlUDv6iKhS",
    "outputId": "632b5ee8-2af6-4eb2-f884-507841d3d620"
   },
   "outputs": [],
   "source": [
    "# Calculate mean rating of all movies and check the popular high rating movies\n",
    "data.groupby('title')['rating'].mean().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beadbf16"
   },
   "source": [
    "The movies have now been sorted according to the ascending order of their ratings. However, there is a problem. A movie can make it to the top of the above list even if only a single user has given it five stars. Therefore, the above stats can be misleading. Normally, a movie which is really a good one gets a higher rating by a large number of users.\n",
    "\n",
    "Let's now observe the total number of ratings for a movie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdKpatZJTab7"
   },
   "source": [
    "Lets explore the # of people who actually gave ratings to a specific movie in \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5363b325",
    "outputId": "eb8e836b-eb3f-4f12-afae-e84ffe1bca16"
   },
   "outputs": [],
   "source": [
    "# Calculate count rating of all movies\n",
    "data.groupby('title')['rating'].count().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68feedf0"
   },
   "source": [
    "Now you can see some really good movies at the top. The above list supports our point that good movies normally receive higher ratings. Now we know that both the average rating per movie and the number of ratings per movie are important attributes. Let's create a new dataframe that contains both of these attributes.\n",
    "\n",
    "Execute the following script to create ratings_mean_count dataframe and first add the average rating of each movie to this dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8Ls8DbpbUy-"
   },
   "source": [
    " Let us create a dataframe with 'rating' count values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlPY2-y-bfLv",
    "outputId": "d56f012a-e804-4df3-91b3-aa4ccf58f027"
   },
   "outputs": [],
   "source": [
    "New_data = pd.DataFrame(data.groupby('title')['rating'].mean())\n",
    " \n",
    "New_data['num of ratings'] = pd.DataFrame(data.groupby('title')['rating'].count())\n",
    " \n",
    "New_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aedca5d"
   },
   "source": [
    "You can see movie title, along with the average rating and number of ratings for the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RikD19U8g1Ka",
    "outputId": "ca75b40a-8b92-4121-cedd-9fbcd85a88d6"
   },
   "outputs": [],
   "source": [
    "#plot rounded-up ratings with number of movies\n",
    "plt.figure(figsize =(15, 10))\n",
    "ax=plt.barh(New_data['rating'].round(),New_data['num of ratings'],color='indigo')\n",
    "plt.xlabel('counts', size=15)\n",
    "plt.ylabel('rounded-up ratings', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK5HGENCiu3J",
    "outputId": "65484f08-26a0-45ab-b4a6-5f2a997041a2"
   },
   "outputs": [],
   "source": [
    "#a bar graph descibibg number of reviews for first 25 movies\n",
    "plt.figure(figsize =(15, 10))\n",
    "ax=plt.subplot()\n",
    "ax.bar(New_data.head(25).index,New_data['num of ratings'].sort_values(ascending=False).head(25),color='indigo')\n",
    "ax.set_xticklabels(New_data.index,rotation=30,fontsize='12',horizontalalignment=\"right\")\n",
    "ax.set_title(\"Total Number of reviews for each movie\", size=20)\n",
    "plt.xlabel('title of movie', size=15)\n",
    "plt.ylabel('Number of reviews', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLq1wQGBcyzZ"
   },
   "source": [
    "Let us now explore the bar plot of '# of people rated feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImN1bRSReZzv",
    "outputId": "2cdf2715-a6f0-43fe-815a-769a4e8388f2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "plt.xlabel('ratings', size=15)\n",
    "plt.ylabel('counts', size=15)\n",
    "New_data['num of ratings'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e26a8fb9"
   },
   "source": [
    "From the output, you can see that most of the movies have received less than 1500 ratings. While the number of movies having more than 3000 ratings is very low.\n",
    "\n",
    "Now we'll plot a histogram for average ratings. Here is the code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQvp0BO8i1vu",
    "outputId": "05ec6963-dc58-43f6-9524-c111fd7dfcdb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "plt.xlabel('ratings', size=15)\n",
    "plt.ylabel('frequency', size=15)\n",
    "New_data['rating'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rcA5l-kMAC-"
   },
   "source": [
    "Lets now explore the most rated movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbQGzIEZoDWs",
    "outputId": "7d059f9a-f88b-43ab-dff7-bde3860f6906"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "sns.jointplot(x=New_data['rating'],y=New_data['num of ratings'], color=\"indigo\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o2UZNtGpkKI"
   },
   "source": [
    "In the above joinplot it is clearly depicted that the movies with rating range from roughly 2 to 4 has the most number of ratings. Also, movies with about 5 rating has around 1 (No. of people rated) to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h5rIt_qpkHQ"
   },
   "source": [
    "Lets just plot a Densityplot to have a look at the dense regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMqUM4vHsLF6",
    "outputId": "02112961-0253-4f87-950a-820c7728f4e5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "sns.jointplot(x=New_data['rating'],y=New_data['num of ratings'],kind='kde', color=\"indigo\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCh8mm-kvyzX"
   },
   "source": [
    "Notice the dark black zone in the above density plot which represent most no. of datapoints almost have 2 to 4 star ratings. Dark regions generally represent the Dense region, which simply means alot of datapoints at that region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmTgAiNtZWIH"
   },
   "source": [
    "Now let see the individual distributions of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqqT5B7MZZ8U",
    "outputId": "40830dbd-cc21-44b0-f5b4-a9ca8ea366a9"
   },
   "outputs": [],
   "source": [
    "sns.distplot(New_data['rating'], color=\"indigo\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6XEoYexaoQ9"
   },
   "source": [
    "It nearly have a Normal distribution with some negative skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jJjrAsK9GDr"
   },
   "source": [
    "**Visualising the most popular Genres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BPHheRPcbJT",
    "outputId": "329b283d-3ce3-475f-bc80-dfbf158b52a2"
   },
   "outputs": [],
   "source": [
    "genres = pd.DataFrame(data['genres'].\n",
    "                      str.split(\"|\").\n",
    "                      tolist(),\n",
    "                      index=data['movieId']).stack()\n",
    "genres = genres.reset_index([0, 'movieId'])\n",
    "genres.columns = ['movieId', 'Genre']\n",
    "genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MV9Lhzxdc0k6",
    "outputId": "b331e106-894d-4089-f0f0-a7ec31b640d5"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7) , dpi=95)\n",
    "sns.countplot(x='Genre',\n",
    "              data=genres,\n",
    "              palette='viridis',\n",
    "              order=genres['Genre'].\n",
    "              value_counts().index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Genre', size=15)\n",
    "plt.ylabel('Count', size=15)\n",
    "plt.title('Distribution of Movie Genres', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6090RTY3NPT9"
   },
   "source": [
    "Movies having the genre as Drama are maximum in number as compared to Film-Noir movies and Documentary movies. A\n",
    "movie might have multipe genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ap85DqOQjkCo",
    "outputId": "6498ec71-70d8-4313-94e8-991d401a8156"
   },
   "outputs": [],
   "source": [
    "# Plotting top 10 movie directors using a count-plot\n",
    "plt.figure(figsize = (14,7))\n",
    "director=data['director']\n",
    "axes=sns.countplot(y=director, order = director.value_counts().index[1:11],color='indigo')\n",
    "axes.set_title('Top 10 Most Popular Movie Directors',fontsize=20)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Count', size=15)\n",
    "plt.ylabel('Director', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4xLaPAdG0uo",
    "outputId": "3d939245-67f4-4ba1-b192-8e6b8c15d25a"
   },
   "outputs": [],
   "source": [
    "# Plotting total amount of movies released in each year using a count plot.\n",
    "figure= plt.subplots(figsize=(14, 7))\n",
    "axes=sns.countplot(x=data['release_year'], order = data['release_year'].value_counts()[0:50].index,color='red')\n",
    "axes.set_title('Total movies released per year',fontsize=20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Count', size=15)\n",
    "plt.xlabel('Release year', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d43cdcb"
   },
   "source": [
    "<a id=\"eight\"></a>\n",
    "# 7. Model Creation\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, Recommender models are created inorder to accurately provide movie content. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9d3ab56"
   },
   "source": [
    "### Filtering strategies\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe594373"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/Types%20of%20Recommenders.PNG?token=GHSAT0AAAAAABXBQYEHY3QEGRHYHIGMJLSWYXCVWPQ\" width=1000 height=1000 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fda39735"
   },
   "source": [
    " ⚡7.1. Content Based filtering ⚡ \n",
    " --------------------------- \n",
    "This filtering strategy provides movie recommendations based on the data provided about the item. The algorthim takes in data regarding movies that the users has liked in the past and provides new movie recommendations based on his/her past likes. The typical type of input data used by the algorthim are:\n",
    "* Lead actor/actress\n",
    "* Director\n",
    "* Genre\n",
    "* Production house\n",
    "* Key words\n",
    "* Cast \n",
    "\n",
    "..etc\n",
    "\n",
    "##### Disadvantages of Content based filtering\n",
    "1. Different products do not get much exposure to the user\n",
    "2. Business expansion is limited since users will not get\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74509b86"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/ContentBased.PNG?token=GHSAT0AAAAAABXBQYEHLODLC6KAQLIECETKYXCVW5A\" width=500 height=300 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb9c682a",
    "outputId": "1c64f8e3-c44b-444c-a43f-fbcf1b69d834"
   },
   "outputs": [],
   "source": [
    "#Getting the percentage count of each rating value \n",
    "count_ratings = train.groupby('rating').count()\n",
    "count_ratings['perc_total']=round(count_ratings['userId']*100/count_ratings['userId'].sum(),1)\n",
    "count_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55b5b7d3",
    "outputId": "860bef5e-2d50-4d2f-ae45-3e151ca94439"
   },
   "outputs": [],
   "source": [
    "#Visualising the percentage total for each rating\n",
    "count_ratings['perc_total'].plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7fcea88",
    "outputId": "7b71ce35-d133-4693-dbfd-605798b61001"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing for Content based Filtering\n",
    "\n",
    "# Split year from title, split genres and clean up data in movies dataframe\n",
    "df_movies = pd.read_csv('../input/edsa-movie-recommendation-2022/movies.csv')\n",
    "df_movies['title']=df_movies['title'].str.split('(')\n",
    "df_movies['genres']=df_movies['genres'].str.split('|')\n",
    "df_movies['Year']=df_movies['title'].str[1]\n",
    "df_movies['title']=df_movies['title'].str[0]\n",
    "df_movies['Year']=df_movies['Year'].str.replace(')','',regex=True)\n",
    "\n",
    "# Remove end spaces\n",
    "def remove_end_spaces(string):\n",
    "    return \"\".join(string.rstrip())\n",
    "df_movies['title']=df_movies['title'].apply(remove_end_spaces)\n",
    "\n",
    "# Split cast and keywords and clean up data in imdb_data dataframe\n",
    "imdb_data = pd.read_csv('../input/edsa-movie-recommendation-2022/imdb_data.csv')\n",
    "imdb_data['title_cast']=imdb_data['title_cast'].str.split('|')\n",
    "imdb_data['plot_keywords']=imdb_data['plot_keywords'].str.split('|')\n",
    "\n",
    "\n",
    "content_input_data_df=''\n",
    "content_input_data_df=imdb_data.merge(df_movies,on='movieId')\n",
    "content_input_data_df['director'] = content_input_data_df['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\n",
    "content_input_data_df['director'] = content_input_data_df['director'].apply(lambda x: [x,x, x])\n",
    "\n",
    "content_input_data_df['input'] = content_input_data_df['plot_keywords'] + content_input_data_df['title_cast'] + content_input_data_df['director'] + content_input_data_df['genres']\n",
    "content_input_data_df=content_input_data_df.dropna(subset=['input'])\n",
    "content_input_data_df['input'] = content_input_data_df['input'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "710470ed",
    "outputId": "faaaca19-4f1a-40f6-f8a1-222e9acc3e16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_input_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec7586d2"
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "count_matrix = count.fit_transform(content_input_data_df['input'])\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d7acacc"
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(np.array(list(range(0, content_input_data_df.shape[0]))), index=content_input_data_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad2dd69b"
   },
   "outputs": [],
   "source": [
    "def get_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:31]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return content_input_data_df['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2236fc36",
    "outputId": "a1bae5de-c69e-4bb9-a427-fb6e6b89edd3"
   },
   "outputs": [],
   "source": [
    "get_recommendations('Toy Story').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c541bf5"
   },
   "source": [
    "### Analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d390364e"
   },
   "source": [
    "Looking at the above Recommendations for the movie 'Toy story', shows that the recommender system does provide a variety of movie suggestions which are in the same/similar genre as 'Toy story'. This model takes into account the Cast, Directors and Plot Key words.\n",
    "\n",
    "The content based filtering however suffers from several limitations. It only suggests movies that are similar to each other on the specified set of variables used (Genre,Cast, Directors and Plot Key words). It does not allow for customized recommendations and personalized suggestions for each unique user. This recommendation system will provide the same recommendation for ANY user who watched a certain movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2221cd52"
   },
   "source": [
    " ⚡7.2. Collaborative Based filtering ⚡ \n",
    " --------------------------- \n",
    " \n",
    " Collaborative filtering is based on the combination of the users behavior and comparing and constrasting that with other users behavior in the database. The main difference between CONTENT based filtering and COLLABORATIVE based filtering is that the recommendations from collaborative based filtering are influenced by all users ratings of items while content based filtering mkaes recommendations using only the users data (and ratings)\n",
    " \n",
    " There are 2 types of Collaborative based filtering: \n",
    " 1. User based collaborative filtering\n",
    " \n",
    " This strategy finds and groups users that have similar past preference patterns.For example: If user A likes Avengers, Dr. Strange and Spiderman, while user B likes Spiderman, Thor and Avengers, this algorthim would identify that these users have similar preference. Hence the recommender system will recommend \"Thor\" to User A, and \"Dr. Strange\" to User B\n",
    " \n",
    "###### Disadvantages of using User based collaborative filtering are:\n",
    " * Similarity patterns between users may no longer be relevant since user preferences may change and as this algorthim is based on user similarity.\n",
    " * There are generally more users and Movie items which significantly increases computational requirements and maintenance. \n",
    " * Prone to Shilling attacks- This is when fake user profiles are created consisting of biased preference patterns which are then used to manipulate the algorthim. \n",
    " \n",
    "\n",
    "\n",
    "2. Item based collaborative filtering\n",
    "\n",
    "This filtering strategy finds similar movies instead of similar users. For example if we take 2 movies A and B, and check their ratings by all users who rated both the movies, and based on these ratings and similarities of users who rated both movies A and B, a recommendation will be made.\n",
    "\n",
    "For example if Movies A and B has an average ration of 9 and 9.5 respectively, we can conclude that both these movies are similar and if a user provides a movie rating of 8.5 for movie A, he/she should be recommended to watch movie B considering they have similar ratings. \n",
    "\n",
    "###### Advantages of Item based collaborative filtering\n",
    "* Movies do not change unlike peoples taste/preferences.\n",
    "* There are less items than people hence lower computational requirements and maintenance.\n",
    "* More robust against Shilling attaks since items cannot be fake (in comparasion to fake user accounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd47c2d5"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/Collaborative.PNG?token=GHSAT0AAAAAABXBQYEH43EHGNNEXVZ5K35OYXCVY2A\" width=500 height=300 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8219d7f1"
   },
   "outputs": [],
   "source": [
    "### Code for Collaborative based filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79e36e33"
   },
   "source": [
    "Singular value decomposition (SVD) is a collaborative filtering method for movie recommendation. The aim for the code implementation is to provide users with movies’ recommendation from the latent features of item-user matrices. SVD is a method from linear algebra that has been generally used as a dimensionality reduction technique in machine learning. SVD is a matrix factorisation technique, which reduces the number of features of a dataset by reducing the space dimension from N-dimension to K-dimension (where K<N). In the context of the recommender system, the SVD is used as a collaborative filtering technique. It uses a matrix structure where each row represents a user, and each column represents an item. To find out more regarding SVD, please read the article of Singular Value Decomposition (SVD) & Its Application In Recommender System: https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8290bb50"
   },
   "outputs": [],
   "source": [
    "reader = Reader() #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1b83475"
   },
   "outputs": [],
   "source": [
    "#%%timeit -r 3 -n 10000\n",
    "data = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader) #use only 5000 data points for the start\n",
    "svd = SVD()\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76ecd978"
   },
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "trainset, testset = train_test_split(data, test_size=0.20)\n",
    "#Training the algorithm on  the trainset\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions for the testset\n",
    "predictions = svd.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then calculate MAE\n",
    "svd_mae = accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"model_type\": \"svd\",\n",
    "          \"stratify\": True\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'RMSE':svd_rmse,'MAE':svd_mae }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65331e5c"
   },
   "outputs": [],
   "source": [
    "#Inputs are User ID and Movie ID\n",
    "svd.predict(1, 1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "879380af"
   },
   "outputs": [],
   "source": [
    "help(svd.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f6cadb3"
   },
   "source": [
    " ⚡7.3. Hybrid Recommendation System ⚡ \n",
    " --------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46532318"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/Hybrid.PNG?token=GHSAT0AAAAAABXBQYEGLQSK45DRSUS4NYC4YXCVZUA\" width=1000 height=1000 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9979b4b"
   },
   "source": [
    "Hybrid Recommendation systems is a special type of recommendation system which is a combination of the content and collaborative filtering methodologies. This methodology together helps overcomes the shortcoming of both content and collaborative filtering when applied separately. Several studies have shown that hybrid models generate more accurate recommendations as compared to conventional approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daf161fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_links=links\n",
    "df_link_hybrid=df_links[['movieId','tmdbId']]\n",
    "df_link_hybrid['tmdbId']=df_link_hybrid['tmdbId'].apply(convert_int)\n",
    "df_link_hybrid=df_link_hybrid.merge(content_input_data_df[['title','movieId']],on='movieId').set_index('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e36d8fa"
   },
   "outputs": [],
   "source": [
    "df_link_hybrid_indices_map=df_link_hybrid.set_index('tmdbId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b872adb2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def hybrid(userId, title):\n",
    "    idx = indices[title]\n",
    "    \n",
    "    sim_scores = list(enumerate(cosine_sim[int(idx)]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:26]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "  \n",
    "    \n",
    "    movies = content_input_data_df.iloc[movie_indices][['title','movieId','Year']]\n",
    "    movieID=np.array(movies['movieId'])\n",
    "\n",
    "    rating_predict=[0] * len(movieID)\n",
    "\n",
    "    for x in range(len(movieID)):\n",
    "        rating_predict[x]=svd.predict(userId, movieID[x])[3]\n",
    "    \n",
    "    #print(rating_predict)\n",
    "    movies['Estimated Rating']=rating_predict\n",
    "    movies = movies.sort_values('Estimated Rating', ascending=False)\n",
    "    return movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bf0d9c7"
   },
   "outputs": [],
   "source": [
    "hybrid(1, 'Toy Story')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb15cb57"
   },
   "source": [
    "<a id=\"nine\"></a>\n",
    "# 8.  Model Improvements\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df514951"
   },
   "source": [
    "*  Account for Genome data and Relevance scores\n",
    "Only tags greater than 0.8 were considered. All tags with a relevance score greater than 0.8 was added to the content based maxtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "325ac4e0"
   },
   "outputs": [],
   "source": [
    "content_input_data_df.drop(['input'],axis=1,inplace=True)\n",
    "x=genome_score[genome_score['relevance']>0.8].sort_values('relevance',ascending=False).groupby('movieId').head(3)\n",
    "x=x.merge(genome_tags,on='tagId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "143125a8"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71dbf3aa"
   },
   "outputs": [],
   "source": [
    "x['tag'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7e8304f"
   },
   "source": [
    "There is a total of 38 492 tags but only 1005 of these tags are unique. This means that there are many tags that are duplicates of each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b35e47b7"
   },
   "outputs": [],
   "source": [
    "x=x.sort_values('movieId').reset_index()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "491eeabc"
   },
   "outputs": [],
   "source": [
    "content_input_data_df=content_input_data_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6e1fd93"
   },
   "outputs": [],
   "source": [
    "genomes=[]\n",
    "content_input_data_df['Genomes'] = \" \"\n",
    "content_input_list=content_input_data_df['movieId'].shape[0]\n",
    "Genome_list=x['tag'].shape[0]\n",
    "Genome_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d32d0c4f"
   },
   "outputs": [],
   "source": [
    "\n",
    "for content_x in range(0,content_input_list):\n",
    "    genomes=[]\n",
    "    for genome_x in range(0,Genome_list): \n",
    "        if content_input_data_df['movieId'][content_x]==x['movieId'][genome_x]:\n",
    "            #print(content_x,' ',genome_x)\n",
    "            genomes.append(x['tag'][genome_x])\n",
    "            content_input_data_df['Genomes'][content_x]= genomes\n",
    "content_input_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb2d2c80"
   },
   "outputs": [],
   "source": [
    "content_input_data_df['Genomes'] = content_input_data_df['plot_keywords']+ content_input_data_df['title_cast'] + content_input_data_df['director'] + content_input_data_df['genres']#+content_input_data_df['Genomes']\n",
    "content_input_data_df=content_input_data_df.dropna(subset=['Genomes'])\n",
    "content_input_data_df['Genomes'] = content_input_data_df['Genomes'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4833a04"
   },
   "source": [
    "Content Based filtering improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7be7be6f"
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "count_matrix = count.fit_transform(content_input_data_df['Genomes'])\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "358177bd"
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(np.array(list(range(0, content_input_data_df.shape[0]))), index=content_input_data_df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc3fc14e"
   },
   "outputs": [],
   "source": [
    "avg_user_rating = pd.DataFrame(train.groupby('userId')['rating'].agg(['mean','count']).sort_values(by=['count'],ascending=False))\n",
    "avg_user_rating.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6009f151"
   },
   "source": [
    "The above table shows the average user ratings and the number of movies they rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a762b278"
   },
   "outputs": [],
   "source": [
    "# Finding the average rating for movie and the number of ratings for each movie\n",
    "avg_movie_rating = pd.DataFrame(train.groupby('movieId')['rating'].agg(['mean','count']))\n",
    "avg_movie_rating['movieId']= avg_movie_rating.index\n",
    "avg_movie_rating.drop(['movieId'],axis=1,inplace=True)\n",
    "avg_movie_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0bbc77"
   },
   "source": [
    "The above table shows the average movie ratings and the number of users who watched those movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0294d0eb"
   },
   "outputs": [],
   "source": [
    "avg_movie_rating.columns = ['Average Rating', 'Total Votes']\n",
    "avg_movie_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06d9afb7"
   },
   "source": [
    "The above table shows the Average rating per movie and total votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d5507c4"
   },
   "outputs": [],
   "source": [
    "#Get the average movie rating across all movies \n",
    "avg_rating_all=train['rating'].mean()\n",
    "avg_rating_all\n",
    "\n",
    "#set a minimum threshold for number of reviews that the movie has to have\n",
    "min_reviews=30\n",
    "min_reviews\n",
    "movie_score = avg_movie_rating.loc[avg_movie_rating['Total Votes']>min_reviews]\n",
    "movie_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b38d2a4"
   },
   "source": [
    "The Average rating and Total votes for each movie is filtered by having a minimum threshold of 30 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0537ac2e"
   },
   "source": [
    "#### Weighted Rating Calculation\n",
    "The formula for calculating the Top Rated 250 Titles gives a true Bayesian estimate:\n",
    "\n",
    "weighted rating (WR) = (v ÷ (v+m)) × R + (m ÷ (v+m)) × C where:\n",
    "\n",
    "\n",
    "R = average for the movie (mean) = (Rating)\n",
    "\n",
    "v = number of votes for the movie = (votes)\n",
    "\n",
    "m = minimum votes required to be listed in the Top 250 \n",
    "\n",
    "C = the mean vote across the whole report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c84edc7b"
   },
   "outputs": [],
   "source": [
    "def weighted_rating(x, m=min_reviews, C=avg_rating_all):\n",
    "    v = x['Total Votes']\n",
    "    R = x['Average Rating']\n",
    "    # Calculation based on the IMDB formula\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dbe01c3"
   },
   "outputs": [],
   "source": [
    "movie_score['weighted_score_rating'] = movie_score.apply(weighted_rating, axis=1)\n",
    "movie_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0200b9c5"
   },
   "source": [
    "The weighted average takes into account the Total votes per movie and Average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7359d109"
   },
   "outputs": [],
   "source": [
    "content_input_data_new_df=content_input_data_df.merge(movie_score,on='movieId',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65751365"
   },
   "outputs": [],
   "source": [
    "content_input_data_new_df=content_input_data_new_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "529694c1"
   },
   "outputs": [],
   "source": [
    "def convert_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "df_links=links\n",
    "df_link_hybrid=df_links[['movieId','tmdbId']]\n",
    "df_link_hybrid['tmdbId']=df_link_hybrid['tmdbId'].apply(convert_int)\n",
    "df_link_hybrid=df_link_hybrid.merge(content_input_data_new_df[['title','movieId']],on='movieId').set_index('title')\n",
    "df_link_hybrid_indices_map=df_link_hybrid.set_index('tmdbId')\n",
    "\n",
    "def hybrid(userId, title):\n",
    "    idx = indices[title]\n",
    "    \n",
    "    sim_scores = list(enumerate(cosine_sim[int(idx)]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:26]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "  \n",
    "    \n",
    "    movies = content_input_data_new_df.iloc[movie_indices][['title','movieId','Year','Total Votes','weighted_score_rating']]\n",
    "    movieID=np.array(movies['movieId'])\n",
    "\n",
    "    rating_predict=[0] * len(movieID)\n",
    "\n",
    "    for x in range(len(movieID)):\n",
    "        rating_predict[x]=svd.predict(userId, movieID[x])[3]\n",
    "    \n",
    "    #print(rating_predict)\n",
    "    movies['Estimated Rating']=rating_predict\n",
    "    movies = movies.sort_values('Estimated Rating', ascending=False)\n",
    "    return movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b524baa"
   },
   "outputs": [],
   "source": [
    "hybrid(1, 'Toy Story')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91b486c3"
   },
   "source": [
    "The hybrid model produces a much better recommendation list by looking at movie content as well as a Estimated ranking using SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feac185a"
   },
   "source": [
    "<a id=\"ten\"></a>\n",
    "# 9.  Model Evaluation |\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e50d61d"
   },
   "source": [
    "Since the Recommender system is an algorithm that returns a ranked order of items, each item is either hit or miss (like relevant vs. irrelevant search results) and items further down in the list are less likely to be viewed/watched. \n",
    "A user has a finite amount of time and attention, so we want to know not just five products users might like, but also which are most liked or which we are most confident of. This lets you show the top recommendations first and maybe market them more aggressively.\n",
    "\n",
    "Mean Average Precision (MAP) is very popular evaluation metric for algorithms that do information retrieval like google search results, but it also can apply to user-targeted product recommendations like what movie one should watch.Using MAP to evaluate a recommender algorithm implies that you are treating the recommendation like a ranking task. This often makes perfect sense since a user has a finite amount of time and attention and we want to show the top recommendations first and maybe market them more aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72f5a5a0"
   },
   "source": [
    "* Evaluate Model performance by changing the \"UserId\" and \"Movie Title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d256d17a"
   },
   "outputs": [],
   "source": [
    "UserId_input=72315\n",
    "Movie_title_input='Jumanji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf0c2129"
   },
   "outputs": [],
   "source": [
    "Prediction=hybrid(UserId_input, Movie_title_input)\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1da9ee5"
   },
   "source": [
    "For User 72315,who has recently watched 'Jumanji',the top 10 recommendations based on Estimated Ratings are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75c642d2"
   },
   "source": [
    "Mean average Precision is calculated as shown in the formula below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de5d5b64"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/Evaluation.PNG?token=GHSAT0AAAAAABXBQYEGJQJCNMYBBA2525I2YXCV3UA\" width=500 height=500 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f29f8ffc"
   },
   "source": [
    "* Number of Our recommendations that are relevant = This refers to all movie recommendations that the User actually watched\n",
    "* Number of items we recommend= This value is currently set at 10\n",
    "* Number of all possible relevant items=This refers to all movies that were watched by the user\n",
    "\n",
    "Keep note that this is a backwardlooking model i.e. The mean average precision is calculated once recommendations are made, and the MAP is calculated based on how many movies of our recommendation did the user watch. \n",
    "\n",
    "The MAP@K metric is the most commonly used metric for evaluating recommender systems. The formula to calculate Mean Average precision (MAP) is :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34c146e7"
   },
   "source": [
    "<br />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RogerA11/TeamCBB5/main/AveragePrecision.PNG?token=GHSAT0AAAAAABXBQYEHWVJPMFAW5QEWUKWGYXCV4NQ\" width=500 height=500 />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31ca3ae6"
   },
   "source": [
    "...where P(k) is the precision value at the kth recommendation, rel(k), is just an indicator that says whether that kth recommendation was relevant (rel(k)=1) or not (rel(k)=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77c2e7bd"
   },
   "outputs": [],
   "source": [
    "#Calculating # of all possible relevant Items\n",
    "NUM_ACTUAL_MOVIES_WATCHED=train[train['userId']==UserId_input].shape[0]\n",
    "NUM_ACTUAL_MOVIES_WATCHED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33d81eee"
   },
   "source": [
    "User 72315 watched and rated 12952 movies. Hence all 12952 movies are considered relevant items to user 72315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f4ba136"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Prediction_movieId=Prediction['movieId'].values\n",
    "Actual_movieId=train[train['userId']==UserId_input]['movieId'].values\n",
    "Prediced_movies_watched=[]\n",
    "\n",
    "\n",
    "recoms=np.zeros(len(Prediction_movieId))\n",
    "recoms=[]\n",
    "for x in range(len(Prediction_movieId)):\n",
    "    if Prediction_movieId[x] in Actual_movieId:\n",
    "        recoms.append(1)\n",
    "        Prediced_movies_watched.append(Prediction_movieId[x])\n",
    "    else:\n",
    "        recoms.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed55f74f"
   },
   "outputs": [],
   "source": [
    "recoms\n",
    "recoms_str=[]\n",
    "correct_recoms=0\n",
    "MAP=0\n",
    "for count,x in enumerate(range(len(recoms))):\n",
    "    if recoms[x]==1:\n",
    "        correct_recoms=correct_recoms+1\n",
    "        MAP=MAP+(correct_recoms)/(count+1)\n",
    "        recoms_str.append('Correct Postive')\n",
    "\n",
    "        \n",
    "    else:\n",
    "        recoms_str.append('False Postive')\n",
    "MAP=round(MAP/sum(recoms),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "824b6bd8"
   },
   "outputs": [],
   "source": [
    "Prediction['Relevance']=recoms_str\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e357fd43"
   },
   "outputs": [],
   "source": [
    "precs = []\n",
    "recalls = []\n",
    "NUM_ACTUAL_MOVIES_WATCHED=10\n",
    "for indx, rec in enumerate(recoms):\n",
    "    precs.append(sum(recoms[:indx+1])/(indx+1))\n",
    "    recalls.append(sum(recoms[:indx+1])/NUM_ACTUAL_MOVIES_WATCHED)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recalls, precs, markersize=10, marker=\"o\")\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"P(i) vs. r(i) for Increasing $i$ for AP@7\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The mean average precision for our recommender system is =\", MAP*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "066bfca2"
   },
   "source": [
    "From the 10 movies recommended, 5 of our movies recommended was relevant and hence watched by  User 72315. The Mean average precision for our current recommender system is 59%. The aim of the MAP score is to show how accurate the recommender system is in ranking the most relevant content higher up in the recommendation list for the user. \n",
    "\n",
    "\n",
    "A great recommender system makes both relevant and useful recommendations. Using a combination of multiple evaluation metrics, we can start to assess the performance of a model by more than just relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d485af9"
   },
   "source": [
    "<a id=\"eleven\"></a>\n",
    "# 10. Discuss chosen methods logic\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af816ac2"
   },
   "source": [
    "\n",
    "\n",
    "* For Model development 3 Models were trialed; [1] Content Based Filtering, [2] Collaborative Based Filtering, and [3] Hybrid Filtering. The Hybrid model takes both; movie content from the content based filtering algorthim and user rating similarities from the collaborative based filtering algorthim and provides a list of Movie recommendations.\n",
    "\n",
    "* A simplified approach was taken to account for Genome tags. Genome tags with relevance scores >0.85 was included to the content based filtering algorthim. \n",
    "\n",
    "* The model was evaluated using the Mean average precision metric since this metric analyzes the relevance of the recommendation list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d600ae53"
   },
   "source": [
    "<a id=\"twelve\"></a>\n",
    "# 11. Conclusions\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33296557"
   },
   "source": [
    "The hybrid model provided the best recommendations since it accounted for both movie content and user rating similarities. Recommendations were ranked based on Estimated ratings using the SVD model. \n",
    "\n",
    "The model achieved a Mean average precision score of 59%, which mean that from the 10 recommendations suggested from the hybrid recommendation system, 5 of these were relevant to the user. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aab04cc"
   },
   "source": [
    "<a id=\"thirteen\"></a>\n",
    "# 12. Recommendations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "977d8220"
   },
   "source": [
    "Although the model does provide recommendations that Users find relevant, the MAP score can be improved by exploring other models such as Neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd360888"
   },
   "source": [
    "# Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5cc4ae8"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "userID=np.array(test['userId'])\n",
    "movieID=np.array(test['movieId'])\n",
    "\n",
    "rating_predict=[0] * test.shape[0]\n",
    "\n",
    "for x in range(test.shape[0]):\n",
    "    rating_predict[x]=svd.predict(userID[x], movieID[x])[3]\n",
    "    \n",
    "\n",
    "collaborative_submission=test\n",
    "#collaborative_submission['id']=collaborative_submission['userId']+'_'+collaborative_submission['movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "861f7fef"
   },
   "outputs": [],
   "source": [
    "collaborative_submission['id']=collaborative_submission['userId'].astype(str)+'_'+collaborative_submission['movieId'].astype(str)\n",
    "collaborative_submission['rating']=rating_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "787430b7"
   },
   "outputs": [],
   "source": [
    "collaborative_submission.drop(['userId','movieId'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f7802bb"
   },
   "outputs": [],
   "source": [
    "collaborative_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79064d22"
   },
   "outputs": [],
   "source": [
    "collaborative_submission.to_csv('collaborative_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a notebook, we need to indicate that an experiment has finished with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47565e61"
   },
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
